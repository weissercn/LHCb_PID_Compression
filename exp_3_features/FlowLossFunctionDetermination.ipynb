{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis notebook is used to figure out what loss function should be used for the flows. \\nHere we use an optimization model to figure out what optimal solution to our loss function is.\\nIn particular we want to figure out if we can design a loss function that when perfectly \\noptimized and disregarding the continuity enforced by the flow, produces the training data \\ndistribution, but gives more weight to errors in the tails rather than the bulk. \\nWe still haven't found an easy way of doing this.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook is used to figure out what loss function should be used for the flows. \n",
    "Here we use an optimization model to figure out what optimal solution to our loss function is.\n",
    "In particular we want to figure out if we can design a loss function that when perfectly \n",
    "optimized and disregarding the continuity enforced by the flow, produces the training data \n",
    "distribution, but gives more weight to errors in the tails rather than the bulk. \n",
    "We still haven't found an easy way of doing this.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor now, the negative loss is defined as the log probability of the set of PID features \\ngiven the flow, averaged over all training data (within the \"context\" of the auxiliary \\nfeatures).\\nloss = - log_prob(inputs=x, context=y).mean()\\nOverall, flows already do a much better job than GANs at considering the tails. If an \\nevent in the tail occurs in the training data and the model gives it 0 probability, then \\nthe loss will be infinite. GANs on the other hand will not be punished for not generating \\nevents in the tails if I understand correctly. The caveat is that more advanced GANs like \\nthe ones we are using promote the generated samples to be \"diverse\", which effectively \\npushes some of the generated samples into some part of the tails.\\n\\nOne way to make the generation of events more likely is to take all possible observed x \\nvalues and add one (or two, or any real number of) copies of them into the dataset used \\nfor the loss calculation.\\n\\nOn the other hand, I\\'ve been thinking about how to keep the optimal distribution produced \\nby the flow to be the close to the actual distribution of training data, while punishing \\nthe difference between the empirical training data distribution and the flow distribution \\nin the tails more severely than the difference of the distributions in the bulk. One solution \\nwould be to bin the training and flow distributions and punish the square difference with a \\nweight depending on whether this bin is in the bulk or tail. Of course this solution struggles \\nwith the curse of dimensionality. \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For now, the negative loss is defined as the log probability of the set of PID features \n",
    "given the flow, averaged over all training data (within the \"context\" of the auxiliary \n",
    "features).\n",
    "loss = - log_prob(inputs=x, context=y).mean()\n",
    "Overall, flows already do a much better job than GANs at considering the tails. If an \n",
    "event in the tail occurs in the training data and the model gives it 0 probability, then \n",
    "the loss will be infinite. GANs on the other hand will not be punished for not generating \n",
    "events in the tails if I understand correctly. The caveat is that more advanced GANs like \n",
    "the ones we are using promote the generated samples to be \"diverse\", which effectively \n",
    "pushes some of the generated samples into some part of the tails.\n",
    "\n",
    "One way to make the generation of events more likely is to take all possible observed x \n",
    "values and add one (or two, or any real number of) copies of them into the dataset used \n",
    "for the loss calculation.\n",
    "\n",
    "On the other hand, I've been thinking about how to keep the optimal distribution produced \n",
    "by the flow to be the close to the actual distribution of training data, while punishing \n",
    "the difference between the empirical training data distribution and the flow distribution \n",
    "in the tails more severely than the difference of the distributions in the bulk. One solution \n",
    "would be to bin the training and flow distributions and punish the square difference with a \n",
    "weight depending on whether this bin is in the bulk or tail. Of course this solution struggles \n",
    "with the curse of dimensionality. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lmfit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-190c7dfd8f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloadtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlmfit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyomo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lmfit'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import exp, linspace, loadtxt, pi, sqrt\n",
    "from lmfit import Model\n",
    "from scipy.optimize import minimize\n",
    "from pyomo.environ import *\n",
    "import pyomo.environ as pyo\n",
    "from pyomo.opt import SolverFactory\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 4 # number of samples\n",
    "U = 3 # number of unique samples\n",
    "\n",
    "\n",
    "model = pyo.ConcreteModel()\n",
    "\n",
    "model.x = pyo.Var(range(U), bounds=(0,1), domain=pyo.NonNegativeReals)\n",
    "\n",
    "\n",
    "def obj_log(model): #objective\n",
    "    #return  1*model.x[0] + 2*model.x[1]+ 1*model.x[2]\n",
    "    return 1*pyo.log(model.x[0]) + 2*pyo.log(model.x[1])+ 1*pyo.log(model.x[2])\n",
    "\n",
    "#model.OBJ = pyo.Objective(sense=maximize, rule= obj_log)\n",
    "#model.OBJ = pyo.Objective(sense=maximize, expr = sum(model.x[i] for i in range(U)  ))\n",
    "#model.OBJ = pyo.Objective(sense=maximize, expr = 1*model.x[0] + 2*model.x[1]+ 1*model.x[2]  ) #with this objective, generate only the mode 100% of the time\n",
    "model.OBJ = pyo.Objective(sense=maximize, expr = 1*pyo.log(model.x[0]) + 2*pyo.log(model.x[1])+ 1*pyo.log(mode.x[2])) #generated the exact fractions we want\n",
    "\n",
    "model.Constraint1 = pyo.Constraint(expr = sum(model.x[i] for i in range(U)  ) == 1)\n",
    "#model.Constraint1 = pyo.Constraint(expr = 3*model.x[0] + 4*model.x[1] >= 1)\n",
    "\n",
    "np.log(1)\n",
    "\n",
    "solver = SolverFactory('ipopt') # only installed on my personal machine\n",
    "#solver = SolverFactory('gurobi') # cannot do nonlinear optimization\n",
    "#solver = SolverFactory('cplex') # couldn't get to work\n",
    "solver.solve(model)\n",
    "\n",
    "print([model.x[i].value for i in range(U)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
